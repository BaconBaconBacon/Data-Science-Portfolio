{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb6e866-9607-4454-a8a7-8026a7934a6a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "GOAL: To train a model to predict the wild fire risk of properties, using census data as input features and the proximity to fires as a target feature.\n",
    "\n",
    "It would be cool to add in data regarding local climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c435b1fb-b3b7-4d5b-837b-9df9ab020f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import census\n",
    "from census import Census\n",
    "\n",
    "import censusgeocode as cg\n",
    "# import pytidycensus as tc\n",
    "\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import sqlalchemy as sql\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from censusdis.states import ALL_STATES_AND_DC\n",
    "\n",
    "\n",
    "import load_GIS \n",
    "import load_census\n",
    "import load_properties\n",
    "\n",
    "\n",
    "from censusgeocode import CensusGeocode\n",
    "from random_address import real_random_address\n",
    "from sqlalchemy.engine import URL\n",
    "\n",
    "\n",
    "from IPython.display import IFrame\n",
    "# from pygris import tracts\n",
    "from matplotlib.colors import to_hex\n",
    "from scipy.stats import randint, uniform\n",
    "# from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "from shapely import distance\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import make_scorer, mean_poisson_deviance, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "CRS = 5070\n",
    "# ALL_STATES_AND_DC\n",
    "\n",
    "\n",
    "# engine = sql.create_engine('sqlite:///Data/wildfire_risk_project.sqlite')\n",
    "engine = sql.create_engine(\"postgresql+psycopg2://postgres:postgres@localhost:5432/wildfire_risk_project\")\n",
    "\n",
    "conn = engine.connect() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da31db-e875-4578-8aa9-eadf78bcf2a2",
   "metadata": {},
   "source": [
    "# Load Training and Target Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5cb6e-f596-446d-90be-3625594ea51d",
   "metadata": {},
   "source": [
    "## Select Properties of Interest\n",
    "\n",
    "Chosing 300000 properties randomly from US addresses. We will join relevant census data to these addresses. This will probably take awhile, so best to run it overnight.\n",
    "\n",
    "We don't care about the address itself. We add a census identifier called the GEOID which based on the coordinate's state, county, and tract number.\n",
    "\n",
    "Using a package that makes use of the [US Census Geocoder API](https://www.census.gov/programs-surveys/geography/technical-documentation/complete-technical-documentation/census-geocoder.html), requests can be in batches of 10,000.\n",
    "\n",
    "https://pypi.org/project/random-address/\n",
    "\n",
    "TODO: This feels like a very naive approach, atm. Takes about 2s per address. Yuck. Speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62bd8da5-5d4b-40cd-8958-a05bd52cf4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new 'properties' table with 10 entries.\n",
      "[{'geoid': 40136141003005, 'block_id': 3005, 'block_grp': 3, 'tract_id': 614100, 'county_id': 13, 'state_id': 4, 'geom': <POINT (-112.204 33.671)>}, {'geoid': 120150104052008, 'block_id': 2008, 'block_grp': 2, 'tract_id': 10405, 'county_id': 15, 'state_id': 12, 'geom': <POINT (-82.037 26.897)>}, {'geoid': 11010056052004, 'block_id': 2004, 'block_grp': 2, 'tract_id': 5605, 'county_id': 101, 'state_id': 1, 'geom': <POINT (-86.203 32.335)>}, {'geoid': 130510101022013, 'block_id': 2013, 'block_grp': 2, 'tract_id': 10102, 'county_id': 51, 'state_id': 13, 'geom': <POINT (-81.055 32.033)>}, {'geoid': 110010033021006, 'block_id': 1006, 'block_grp': 1, 'tract_id': 3302, 'county_id': 1, 'state_id': 11, 'geom': <POINT (-77.012 38.915)>}, {'geoid': 250092621002016, 'block_id': 2016, 'block_grp': 2, 'tract_id': 262100, 'county_id': 9, 'state_id': 25, 'geom': <POINT (-71.013 42.828)>}, {'geoid': 20200017321000, 'block_id': 1000, 'block_grp': 1, 'tract_id': 1732, 'county_id': 20, 'state_id': 2, 'geom': <POINT (-149.734 61.191)>}, {'geoid': 51430101101003, 'block_id': 1003, 'block_grp': 1, 'tract_id': 10110, 'county_id': 143, 'state_id': 5, 'geom': <POINT (-94.138 36.083)>}, {'geoid': 11010022013005, 'block_id': 3005, 'block_grp': 3, 'tract_id': 2201, 'county_id': 101, 'state_id': 1, 'geom': <POINT (-86.303 32.331)>}, {'geoid': 80590102131008, 'block_id': 1008, 'block_grp': 1, 'tract_id': 10213, 'county_id': 59, 'state_id': 8, 'geom': <POINT (-105.065 39.82)>}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid</th>\n",
       "      <th>block_id</th>\n",
       "      <th>block_grp</th>\n",
       "      <th>tract_id</th>\n",
       "      <th>county_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>geom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40136141003005</td>\n",
       "      <td>3005</td>\n",
       "      <td>3</td>\n",
       "      <td>614100</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>POINT (-112.204 33.671)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120150104052008</td>\n",
       "      <td>2008</td>\n",
       "      <td>2</td>\n",
       "      <td>10405</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>POINT (-82.037 26.897)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11010056052004</td>\n",
       "      <td>2004</td>\n",
       "      <td>2</td>\n",
       "      <td>5605</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-86.203 32.335)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130510101022013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>10102</td>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>POINT (-81.055 32.033)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110010033021006</td>\n",
       "      <td>1006</td>\n",
       "      <td>1</td>\n",
       "      <td>3302</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>POINT (-77.012 38.915)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             geoid  block_id  block_grp  tract_id  county_id  state_id  \\\n",
       "0   40136141003005      3005          3    614100         13         4   \n",
       "1  120150104052008      2008          2     10405         15        12   \n",
       "2   11010056052004      2004          2      5605        101         1   \n",
       "3  130510101022013      2013          2     10102         51        13   \n",
       "4  110010033021006      1006          1      3302          1        11   \n",
       "\n",
       "                      geom  \n",
       "0  POINT (-112.204 33.671)  \n",
       "1   POINT (-82.037 26.897)  \n",
       "2   POINT (-86.203 32.335)  \n",
       "3   POINT (-81.055 32.033)  \n",
       "4   POINT (-77.012 38.915)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = load_properties.Properties(sql_engine = engine, sql_conn = conn)\n",
    "properties.properties_gpd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27612d81-b5c9-4da1-a544-b6d201a1ca38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "518aea75-14ee-42fe-8f34-cc7e12f76fa6",
   "metadata": {},
   "source": [
    "## Load 2023 US Census Data\n",
    "\n",
    "Using an API key, we will use the 'census' Python package to interact with the US Govermnent's census API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e3293c-3d65-4812-99e4-441955b2e575",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CensusData.__init__() missing 1 required positional argument: 'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m census = \u001b[43mload_census\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCensusData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: CensusData.__init__() missing 1 required positional argument: 'year'"
     ]
    }
   ],
   "source": [
    "\n",
    "census = load_census.CensusData(engine, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939442c0-680d-4b3d-a5bd-f6c54e2100b4",
   "metadata": {},
   "source": [
    "## Load Wildfire GIS Data for 2024\n",
    "\n",
    "We will use point data from the Visible Infrared Imaging Radiometer Suite (VIIRS). A valid alternative is using burn boundary data. There are a few different data sources we could use, but in the interest of (portfolio) simplicity we'll use just the VIIRS.\n",
    "\n",
    "N:B: May be a good chance to practice using AWS DB storage and retrieval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18aaba-9311-40e8-b684-ce626f6f42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_filepaths=[]\n",
    "wf_gis = load_GIS.GISData(engine, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99afe9c-9916-458e-9b66-f4b7f18179c0",
   "metadata": {},
   "source": [
    "## Give Each Property a Wildfire Risk Score\n",
    "\n",
    "Will be based on the proximity to wildfire points, weighted by the number of nearby fires, with a cutoff of 50km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a7a0f-9c28-4216-9fb1-554c57395a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0e0ab-8d9c-4656-88b6-42cf3679b945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fcaae-5c84-468e-8380-f14d68ba4e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc078c43-577f-4069-a203-927b88c373e2",
   "metadata": {},
   "source": [
    "# Machine Learning Considerations\n",
    "#### Scoring Methods\n",
    "\n",
    "For the float risk score, we can use Mean Squared Error (MSE) or Root Mean Squared Error (RMSE). Since it's quadratic in difference between observations and predictions deviations, MSE strongly penalizes large misses, which would be expensive for the insurance company.\n",
    "\n",
    "For the risk category counts, they appear to be Poisson distributed, so a Poisson loss-function is appropriate.\n",
    "\n",
    "For any classification model with the binned risk categories, we want to make large misses costly (i.e. predicting a 1 when the category is a 10), since these would also be very costly to the insurance company. To be honest, MSE will work here as well, since the categories are just "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a12bd-9f3a-45a3-9b13-0287ff19c033",
   "metadata": {},
   "source": [
    "# Model Machine Learning\n",
    "\n",
    "\n",
    "NB: A good chance to make use of AWS compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25654e8f-fc82-48df-b55c-27c50f0ffab6",
   "metadata": {},
   "source": [
    "### Split Data into Features/Targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b244b-0cbd-4df6-bcb1-a262e5cde235",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col =[\n",
    "    'within_MTBS', 'within_MADIS', 'within_WFIGS',\n",
    "   'within_ba', 'within_ba_no_WFIGS', 'noaa20_score',\n",
    "   'noaa20_count_within_radius', 'noaa20_avg_dist_km', \n",
    "   'snpp_score', 'risk_score_prediction', 'snpp_count_within_radius',\n",
    "   'id', 'longitude','latitude', 'geometry', 'geo_id', \n",
    "   'STUSPS', 'NAMELSADCO', 'score_cat', 'snpp_avg_dist_km', 'geo_id','geometry'\n",
    "  ]\n",
    "ml_df = model_joined.copy().drop(columns=drop_col)\n",
    "del(model_joined)\n",
    "ml_df._consolidate_inplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b86edc-61bc-4b7b-a7a0-e5604a5ee5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target columns\n",
    "target_col = ml_df['sat_avg']\n",
    "feature_cols = ml_df.drop(columns=['sat_avg'])\n",
    "\n",
    "random_state = 77\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_cols, target_col, test_size=0.2, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886d78d-1b6e-4263-b12a-33f8d046136f",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758a6fc-a2b0-4844-9262-c473c17a48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfr_rand_fp =os.path.join(\"Models\",\"model_pred_best_RFR_randomCV.sav\")\n",
    "\n",
    "# if not os.path.exists(rfr_rand_fp):\n",
    "#     param_dist = {\n",
    "#         \"n_estimators\":    randint(100, 1000),   \n",
    "#         \"max_depth\":       randint(5, 50),       \n",
    "#         \"min_samples_split\": randint(2, 11),    \n",
    "#         \"min_samples_leaf\":  randint(1, 5), \n",
    "#         \"max_features\":    [ \"sqrt\", \"log2\"] \n",
    "#     }\n",
    "    \n",
    "#     rfr = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    "    \n",
    "#     random_srch = RandomizedSearchCV(\n",
    "#         estimator=rfr,\n",
    "#         param_distributions=param_dist,\n",
    "#         n_iter=5,  # start with 20 to get a feel for time\n",
    "#         scoring='neg_mean_squared_error',\n",
    "#         cv=5, \n",
    "#         random_state=random_state,\n",
    "#         n_jobs=-1,\n",
    "#         verbose=5  # 1\n",
    "#     )\n",
    "\n",
    "#     random_srch.fit(X_train, y_train)\n",
    "    \n",
    "#     print('best rfr params:', random_srch.best_params_)\n",
    "#     # print('best score:', -random_srch.best_score_)\n",
    "#     best_rfr = random_srch.best_estimator_\n",
    "    \n",
    "\n",
    "#     pickle.dump(best_rfr, open(rfr_rand_fp, 'wb'))\n",
    "# best_rfr = pickle.load(open(rfr_rand_fp,'rb'))\n",
    "\n",
    "# y_pred = best_rfr.predict(X_train)\n",
    "# print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "\n",
    "# y_pred = best_rfr.predict(X_test)\n",
    "# print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d24f38-5694-4ad1-801e-a5ebd0b50f6d",
   "metadata": {},
   "source": [
    "#### XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e4b0b-e353-47d8-937e-5451a9aa5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_choice = 3\n",
    "n_job_choice = 20\n",
    "\n",
    "xgb_rand_fp =os.path.join(\"Models\",\"model_pred_bbest_XGB_randomCV_{}cv_{}job.sav\".format(cv_choice, n_job_choice))\n",
    "\n",
    "if not os.path.exists(xgb_rand_fp):\n",
    "    \n",
    "    # XGBRegressor\n",
    "    param_dist = {\n",
    "        'n_estimators':randint(100, 1000),\n",
    "        'learning_rate':uniform(0.01, 0.29),\n",
    "        'max_depth':randint(3, 12),\n",
    "        'min_child_weight':randint(1, 10),\n",
    "        'subsample':uniform(0.5, 0.5),\n",
    "        'colsample_bytree':uniform(0.5, 0.5),\n",
    "        'gamma':uniform(0, 0.5),\n",
    "        'reg_alpha': uniform(0, 1),\n",
    "        'reg_lambda':uniform(0, 1),\n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=random_state, n_jobs=-1)\n",
    "    \n",
    "    random_srch = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,  # start with 20 to get a feel for time\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3, #5, \n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbose=5  # 1\n",
    "    )\n",
    "    \n",
    "    random_srch.fit(X_train, y_train)\n",
    "    print('best xgb params:', random_srch.best_params_)\n",
    "    # print('best score:', -random_srch.best_score_)\n",
    "    best_xgb = random_srch.best_estimator_\n",
    "    pickle.dump(best_xgb, open(xgb_rand_fp, 'wb'))\n",
    "    \n",
    "best_xgb = pickle.load(open(xgb_rand_fp,'rb'))\n",
    "y_pred = best_xgb.predict(X_train)\n",
    "print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7198993-04ac-4bfd-b263-7c48227522e6",
   "metadata": {},
   "source": [
    "#### Extract Feature Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02372438-c54c-4e77-8a0e-3cd75cc5de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "booster = best_xgb.get_booster()\n",
    "\n",
    "importance_dict = booster.get_score(importance_type='gain')\n",
    "\n",
    "imp_series = (\n",
    "    pd.Series(importance_dict)\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "imp_arr = pd.Series(best_xgb.feature_importances_, index=X_train.columns)\n",
    "top10 = imp_arr.sort_values(ascending=False).head(10)\n",
    "print(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300ce2a-7c3b-41dd-bd32-a0024b29b4e1",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97417d8-d82a-4d7b-84dd-6d3ea46c2866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
