{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb6e866-9607-4454-a8a7-8026a7934a6a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "GOAL: To train a model to predict the wild fire risk of properties, using census data as input features and the proximity to fires as a target feature.\n",
    "\n",
    "It would be cool to add in data regarding local climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c435b1fb-b3b7-4d5b-837b-9df9ab020f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdcf5\\Documents\\Data Science Portfolio\\Python\\Property Wildfire Risk\\load_census.py:7: UserWarning: Mapping functions unavailable due to import error: NameError. To use mapping features, ensure all dependencies are properly installed: pip install pytidycensus[map]\n",
      "  import pytidycensus as tc\n"
     ]
    }
   ],
   "source": [
    "import census\n",
    "from census import Census\n",
    "\n",
    "import censusgeocode as cg\n",
    "# import pytidycensus as tc\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import sqlalchemy as sql\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from censusdis.states import ALL_STATES_AND_DC\n",
    "\n",
    "\n",
    "import load_GIS \n",
    "import load_census\n",
    "import load_properties\n",
    "\n",
    "\n",
    "from censusgeocode import CensusGeocode\n",
    "from random_address import real_random_address\n",
    "from sqlalchemy.engine import URL\n",
    "\n",
    "\n",
    "from IPython.display import IFrame\n",
    "# from pygris import tracts\n",
    "from matplotlib.colors import to_hex\n",
    "from scipy.stats import randint, uniform\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "from shapely import distance\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import make_scorer, mean_poisson_deviance, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "CRS = 5070\n",
    "# ALL_STATES_AND_DC\n",
    "\n",
    "\n",
    "engine = sql.create_engine('sqlite:///wildfire_risk_project.sqlite')\n",
    "conn = engine.connect() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da31db-e875-4578-8aa9-eadf78bcf2a2",
   "metadata": {},
   "source": [
    "# Load Training and Target Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5cb6e-f596-446d-90be-3625594ea51d",
   "metadata": {},
   "source": [
    "## Select Properties of Interest\n",
    "\n",
    "Chosing 300000 properties randomly from US addresses. We will join relevant census data to these addresses. This will probably take awhile, so best to run it overnight.\n",
    "\n",
    "We don't care about the address itself. We add a census identifier called the GEOID which based on the coordinate's state, county, and tract number.\n",
    "\n",
    "Using a package that makes use of the [US Census Geocoder API](https://www.census.gov/programs-surveys/geography/technical-documentation/complete-technical-documentation/census-geocoder.html), requests can be in batches of 10,000.\n",
    "\n",
    "https://pypi.org/project/random-address/\n",
    "\n",
    "TODO: This feels like a very naive approach, atm. Takes about 2s per address. Yuck. Speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62bd8da5-5d4b-40cd-8958-a05bd52cf4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty 'properties' table.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown column geometry",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'geometry'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\geopandas\\geodataframe.py:362\u001b[39m, in \u001b[36mGeoDataFrame.set_geometry\u001b[39m\u001b[34m(self, col, drop, inplace, crs)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     level = \u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\geopandas\\geodataframe.py:1750\u001b[39m, in \u001b[36mGeoDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[33;03mIf the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[33;03mGeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[32m   1748\u001b[39m \u001b[33;03mreturn a GeoDataFrame.\u001b[39;00m\n\u001b[32m   1749\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# Custom logic to avoid waiting for pandas GH51895\u001b[39;00m\n\u001b[32m   1752\u001b[39m \u001b[38;5;66;03m# result is not geometry dtype for multi-indexes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'geometry'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m properties = \u001b[43mload_properties\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProperties\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_engine\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_conn\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Data Science Portfolio\\Python\\Property Wildfire Risk\\load_properties.py:33\u001b[39m, in \u001b[36mProperties.__init__\u001b[39m\u001b[34m(self, sql_engine, sql_conn)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m.sql_engine = sql_engine\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m.sql_conn = sql_conn\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28mself\u001b[39m.properties_gpd = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect_to_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.num_properties = \u001b[38;5;28mself\u001b[39m.properties_gpd.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Data Science Portfolio\\Python\\Property Wildfire Risk\\load_properties.py:92\u001b[39m, in \u001b[36mProperties._connect_to_sql\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating empty \u001b[39m\u001b[33m'\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m'\u001b[39m\u001b[33m table.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28mself\u001b[39m.properties_gpd = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGeoDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mLABELS_KEYS_MAP\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgeometry\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mself\u001b[39m.properties_gpd.to_postgis(\u001b[38;5;28mself\u001b[39m.TABLE_NAME, con=conn,if_exists=\u001b[33m'\u001b[39m\u001b[33mfail\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\geopandas\\geodataframe.py:209\u001b[39m, in \u001b[36mGeoDataFrame.__init__\u001b[39m\u001b[34m(self, data, geometry, crs, *args, **kwargs)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(geometry, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m geometry.name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    205\u001b[39m         \u001b[38;5;66;03m# __init__ always creates geometry col named \"geometry\"\u001b[39;00m\n\u001b[32m    206\u001b[39m         \u001b[38;5;66;03m# rename as `set_geometry` respects the given series name\u001b[39;00m\n\u001b[32m    207\u001b[39m         geometry = geometry.rename(\u001b[33m\"\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m geometry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m crs:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    213\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAssigning CRS to a GeoDataFrame without a geometry column is not \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    214\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupported. Supply geometry using the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgeometry=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword argument, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor by providing a DataFrame with column name \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\geopandas\\geodataframe.py:364\u001b[39m, in \u001b[36mGeoDataFrame.set_geometry\u001b[39m\u001b[34m(self, col, drop, inplace, crs)\u001b[39m\n\u001b[32m    362\u001b[39m     level = frame[col]\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnknown column \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % col)\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level, DataFrame):\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    367\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGeoDataFrame does not support setting the geometry column where \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe column name is shared by multiple columns.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Unknown column geometry"
     ]
    }
   ],
   "source": [
    "properties = load_properties.Properties(sql_engine = engine, sql_conn = conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518aea75-14ee-42fe-8f34-cc7e12f76fa6",
   "metadata": {},
   "source": [
    "## Load 2023 US Census Data\n",
    "\n",
    "Using an API key, we will use the 'census' Python package to interact with the US Govermnent's census API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10e3293c-3d65-4812-99e4-441955b2e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "census = load_census.CensusData(engine, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939442c0-680d-4b3d-a5bd-f6c54e2100b4",
   "metadata": {},
   "source": [
    "## Load Wildfire GIS Data for 2024\n",
    "\n",
    "We will use point data from the Visible Infrared Imaging Radiometer Suite (VIIRS). A valid alternative is using burn boundary data. There are a few different data sources we could use, but in the interest of (portfolio) simplicity we'll use just the VIIRS.\n",
    "\n",
    "N:B: May be a good chance to practice using AWS DB storage and retrieval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18aaba-9311-40e8-b684-ce626f6f42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_filepaths=[]\n",
    "wf_gis = load_GIS.GISData(engine, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99afe9c-9916-458e-9b66-f4b7f18179c0",
   "metadata": {},
   "source": [
    "## Give Each Property a Wildfire Risk Score\n",
    "\n",
    "Will be based on the proximity to wildfire points, weighted by the number of nearby fires, with a cutoff of 50km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a7a0f-9c28-4216-9fb1-554c57395a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0e0ab-8d9c-4656-88b6-42cf3679b945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fcaae-5c84-468e-8380-f14d68ba4e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc078c43-577f-4069-a203-927b88c373e2",
   "metadata": {},
   "source": [
    "# Machine Learning Considerations\n",
    "#### Scoring Methods\n",
    "\n",
    "For the float risk score, we can use Mean Squared Error (MSE) or Root Mean Squared Error (RMSE). Since it's quadratic in difference between observations and predictions deviations, MSE strongly penalizes large misses, which would be expensive for the insurance company.\n",
    "\n",
    "For the risk category counts, they appear to be Poisson distributed, so a Poisson loss-function is appropriate.\n",
    "\n",
    "For any classification model with the binned risk categories, we want to make large misses costly (i.e. predicting a 1 when the category is a 10), since these would also be very costly to the insurance company. To be honest, MSE will work here as well, since the categories are just "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a12bd-9f3a-45a3-9b13-0287ff19c033",
   "metadata": {},
   "source": [
    "# Model Machine Learning\n",
    "\n",
    "\n",
    "NB: A good chance to make use of AWS compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25654e8f-fc82-48df-b55c-27c50f0ffab6",
   "metadata": {},
   "source": [
    "### Split Data into Features/Targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b244b-0cbd-4df6-bcb1-a262e5cde235",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col =[\n",
    "    'within_MTBS', 'within_MADIS', 'within_WFIGS',\n",
    "   'within_ba', 'within_ba_no_WFIGS', 'noaa20_score',\n",
    "   'noaa20_count_within_radius', 'noaa20_avg_dist_km', \n",
    "   'snpp_score', 'risk_score_prediction', 'snpp_count_within_radius',\n",
    "   'id', 'longitude','latitude', 'geometry', 'geo_id', \n",
    "   'STUSPS', 'NAMELSADCO', 'score_cat', 'snpp_avg_dist_km', 'geo_id','geometry'\n",
    "  ]\n",
    "ml_df = model_joined.copy().drop(columns=drop_col)\n",
    "del(model_joined)\n",
    "ml_df._consolidate_inplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b86edc-61bc-4b7b-a7a0-e5604a5ee5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target columns\n",
    "target_col = ml_df['sat_avg']\n",
    "feature_cols = ml_df.drop(columns=['sat_avg'])\n",
    "\n",
    "random_state = 77\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_cols, target_col, test_size=0.2, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886d78d-1b6e-4263-b12a-33f8d046136f",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758a6fc-a2b0-4844-9262-c473c17a48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfr_rand_fp =os.path.join(\"Models\",\"model_pred_best_RFR_randomCV.sav\")\n",
    "\n",
    "# if not os.path.exists(rfr_rand_fp):\n",
    "#     param_dist = {\n",
    "#         \"n_estimators\":    randint(100, 1000),   \n",
    "#         \"max_depth\":       randint(5, 50),       \n",
    "#         \"min_samples_split\": randint(2, 11),    \n",
    "#         \"min_samples_leaf\":  randint(1, 5), \n",
    "#         \"max_features\":    [ \"sqrt\", \"log2\"] \n",
    "#     }\n",
    "    \n",
    "#     rfr = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    "    \n",
    "#     random_srch = RandomizedSearchCV(\n",
    "#         estimator=rfr,\n",
    "#         param_distributions=param_dist,\n",
    "#         n_iter=5,  # start with 20 to get a feel for time\n",
    "#         scoring='neg_mean_squared_error',\n",
    "#         cv=5, \n",
    "#         random_state=random_state,\n",
    "#         n_jobs=-1,\n",
    "#         verbose=5  # 1\n",
    "#     )\n",
    "\n",
    "#     random_srch.fit(X_train, y_train)\n",
    "    \n",
    "#     print('best rfr params:', random_srch.best_params_)\n",
    "#     # print('best score:', -random_srch.best_score_)\n",
    "#     best_rfr = random_srch.best_estimator_\n",
    "    \n",
    "\n",
    "#     pickle.dump(best_rfr, open(rfr_rand_fp, 'wb'))\n",
    "# best_rfr = pickle.load(open(rfr_rand_fp,'rb'))\n",
    "\n",
    "# y_pred = best_rfr.predict(X_train)\n",
    "# print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "\n",
    "# y_pred = best_rfr.predict(X_test)\n",
    "# print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d24f38-5694-4ad1-801e-a5ebd0b50f6d",
   "metadata": {},
   "source": [
    "#### XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e4b0b-e353-47d8-937e-5451a9aa5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_choice = 3\n",
    "n_job_choice = 20\n",
    "\n",
    "xgb_rand_fp =os.path.join(\"Models\",\"model_pred_bbest_XGB_randomCV_{}cv_{}job.sav\".format(cv_choice, n_job_choice))\n",
    "\n",
    "if not os.path.exists(xgb_rand_fp):\n",
    "    \n",
    "    # XGBRegressor\n",
    "    param_dist = {\n",
    "        'n_estimators':randint(100, 1000),\n",
    "        'learning_rate':uniform(0.01, 0.29),\n",
    "        'max_depth':randint(3, 12),\n",
    "        'min_child_weight':randint(1, 10),\n",
    "        'subsample':uniform(0.5, 0.5),\n",
    "        'colsample_bytree':uniform(0.5, 0.5),\n",
    "        'gamma':uniform(0, 0.5),\n",
    "        'reg_alpha': uniform(0, 1),\n",
    "        'reg_lambda':uniform(0, 1),\n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=random_state, n_jobs=-1)\n",
    "    \n",
    "    random_srch = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,  # start with 20 to get a feel for time\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3, #5, \n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbose=5  # 1\n",
    "    )\n",
    "    \n",
    "    random_srch.fit(X_train, y_train)\n",
    "    print('best xgb params:', random_srch.best_params_)\n",
    "    # print('best score:', -random_srch.best_score_)\n",
    "    best_xgb = random_srch.best_estimator_\n",
    "    pickle.dump(best_xgb, open(xgb_rand_fp, 'wb'))\n",
    "    \n",
    "best_xgb = pickle.load(open(xgb_rand_fp,'rb'))\n",
    "y_pred = best_xgb.predict(X_train)\n",
    "print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7198993-04ac-4bfd-b263-7c48227522e6",
   "metadata": {},
   "source": [
    "#### Extract Feature Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02372438-c54c-4e77-8a0e-3cd75cc5de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "booster = best_xgb.get_booster()\n",
    "\n",
    "importance_dict = booster.get_score(importance_type='gain')\n",
    "\n",
    "imp_series = (\n",
    "    pd.Series(importance_dict)\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "imp_arr = pd.Series(best_xgb.feature_importances_, index=X_train.columns)\n",
    "top10 = imp_arr.sort_values(ascending=False).head(10)\n",
    "print(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300ce2a-7c3b-41dd-bd32-a0024b29b4e1",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97417d8-d82a-4d7b-84dd-6d3ea46c2866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
