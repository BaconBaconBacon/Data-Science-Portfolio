{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb6e866-9607-4454-a8a7-8026a7934a6a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "GOAL: To train a model to predict the wild fire risk of properties, using census data as input features and the proximity to fires as a target feature.\n",
    "\n",
    "It would be cool to add in data regarding local climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c435b1fb-b3b7-4d5b-837b-9df9ab020f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import census\n",
    "import censusgeocode as cg\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from censusdis.states import ALL_STATES_AND_DC\n",
    "\n",
    "\n",
    "import load_GIS \n",
    "import load_census\n",
    "import load_properties\n",
    "\n",
    "\n",
    "from censusgeocode import CensusGeocode\n",
    "from random_address import real_random_address\n",
    "\n",
    "\n",
    "from IPython.display import IFrame\n",
    "# from pygris import tracts\n",
    "from matplotlib.colors import to_hex\n",
    "from scipy.stats import randint, uniform\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "from shapely import distance\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import make_scorer, mean_poisson_deviance, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "CRS = 5070\n",
    "# ALL_STATES_AND_DC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da31db-e875-4578-8aa9-eadf78bcf2a2",
   "metadata": {},
   "source": [
    "# Load Training and Target Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5cb6e-f596-446d-90be-3625594ea51d",
   "metadata": {},
   "source": [
    "## Select Properties of Interest\n",
    "\n",
    "Chosing 300000 properties randomly from US addresses. We will join relevant census data to these addresses. This will probably take awhile, so best to run it overnight.\n",
    "\n",
    "We don't care about the address itself. We add a census identifier called the GEOID which based on the coordinate's state, county, and tract number.\n",
    "\n",
    "Using a package that makes use of the [US Census Geocoder API](https://www.census.gov/programs-surveys/geography/technical-documentation/complete-technical-documentation/census-geocoder.html), requests can be in batches of 10,000.\n",
    "\n",
    "https://pypi.org/project/random-address/\n",
    "\n",
    "TODO: How to speed this up? This feels like a very naive approach, atm. Takes about 2s per address. Yuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8c0dc-0eb1-46c2-b823-56ff9a5a3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_properties = 1000  # 3000000\n",
    "filepath = os.path.join('Data', 'property_coords.shp')\n",
    "start = time.time()\n",
    "\n",
    "# if not os.path.exists(filepath):\n",
    "temp_lst = []\n",
    "for _ in range(num_properties):\n",
    "\n",
    "    coords = real_random_address()['coordinates']\n",
    "    \n",
    "    lat = coords['lat']\n",
    "    long=coords['lng']\n",
    "    resp = cg.coordinates(x=long, y=lat)\n",
    "    # print(resp['Counties'])#.keys())\n",
    "    # raise\n",
    "    geoid = resp['2020 Census Blocks'][0]['GEOID']\n",
    "    county= resp['Counties'][0]['BASENAME']\n",
    "    state_code = resp['States'][0]['STATE']\n",
    "    state_abb = resp['States'][0]['STUSAB']\n",
    "    \n",
    "    temp_lst.append(\n",
    "        {\n",
    "        'geometry' :   Point(long, lat),\n",
    "        'geoid' :geoid,\n",
    "        'county':county,\n",
    "        'state_code':state_code,\n",
    "        'state_abb':state_abb\n",
    "        }\n",
    "       )\n",
    "properties_gpd = gpd.GeoDataFrame(temp_lst, crs=CRS)\n",
    "properties_gpd.to_file(filepath,mode='a')\n",
    "# else:\n",
    "properties_gpd= gpd.read_file(filepath).to_crs(CRS)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "properties_gpd.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243766e3-22ff-4eb4-934a-65963c3e19b8",
   "metadata": {},
   "source": [
    "Building the properties list over time, multiple runs, should drop dupes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd8da5-5d4b-40cd-8958-a05bd52cf4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "518aea75-14ee-42fe-8f34-cc7e12f76fa6",
   "metadata": {},
   "source": [
    "## Load 2023 US Census Data\n",
    "\n",
    "Using an API key, we will use the 'census' Python package to interact with the US Govermnent's census API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e3293c-3d65-4812-99e4-441955b2e575",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LoadCensus.__init__() missing 1 required positional argument: 'filepaths'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m census_df \u001b[38;5;241m=\u001b[39m load_census\u001b[38;5;241m.\u001b[39mLoadCensus()\n\u001b[0;32m      2\u001b[0m gis_df \u001b[38;5;241m=\u001b[39m load_GIS\u001b[38;5;241m.\u001b[39mLoadGIS()\n",
      "\u001b[1;31mTypeError\u001b[0m: LoadCensus.__init__() missing 1 required positional argument: 'filepaths'"
     ]
    }
   ],
   "source": [
    "tracts=properties_gpd['tracts'].drop_duplicates()\n",
    "print(tracts)\n",
    "\n",
    "\n",
    "\n",
    "census = load_census.CensusData(tracts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939442c0-680d-4b3d-a5bd-f6c54e2100b4",
   "metadata": {},
   "source": [
    "## Load Wildfire GIS Data for 2024\n",
    "\n",
    "We will use point data from the Visible Infrared Imaging Radiometer Suite (VIIRS). A valid alternative is using burn boundary data. There are a few different data sources we could use, but in the interest of (portfolio) simplicity we'll use just the VIIRS.\n",
    "\n",
    "N:B: May be a good chance to practice using AWS DB storage and retrieval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18aaba-9311-40e8-b684-ce626f6f42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_filepaths=[]\n",
    "wf_gis = load_GIS.GISData(gis_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99afe9c-9916-458e-9b66-f4b7f18179c0",
   "metadata": {},
   "source": [
    "## Give Each Property a Wildfire Risk Score\n",
    "\n",
    "Will be based on the proximity to wildfire points, weighted by the number of nearby fires, with a cutoff of 50km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a7a0f-9c28-4216-9fb1-554c57395a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0e0ab-8d9c-4656-88b6-42cf3679b945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fcaae-5c84-468e-8380-f14d68ba4e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc078c43-577f-4069-a203-927b88c373e2",
   "metadata": {},
   "source": [
    "# Machine Learning Considerations\n",
    "#### Scoring Methods\n",
    "\n",
    "For the float risk score, we can use Mean Squared Error (MSE) or Root Mean Squared Error (RMSE). Since it's quadratic in difference between observations and predictions deviations, MSE strongly penalizes large misses, which would be expensive for the insurance company.\n",
    "\n",
    "For the risk category counts, they appear to be Poisson distributed, so a Poisson loss-function is appropriate.\n",
    "\n",
    "For any classification model with the binned risk categories, we want to make large misses costly (i.e. predicting a 1 when the category is a 10), since these would also be very costly to the insurance company. To be honest, MSE will work here as well, since the categories are just "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a12bd-9f3a-45a3-9b13-0287ff19c033",
   "metadata": {},
   "source": [
    "# Model Machine Learning\n",
    "\n",
    "\n",
    "NB: A good chance to make use of AWS compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25654e8f-fc82-48df-b55c-27c50f0ffab6",
   "metadata": {},
   "source": [
    "### Split Data into Features/Targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b244b-0cbd-4df6-bcb1-a262e5cde235",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col =[\n",
    "    'within_MTBS', 'within_MADIS', 'within_WFIGS',\n",
    "   'within_ba', 'within_ba_no_WFIGS', 'noaa20_score',\n",
    "   'noaa20_count_within_radius', 'noaa20_avg_dist_km', \n",
    "   'snpp_score', 'risk_score_prediction', 'snpp_count_within_radius',\n",
    "   'id', 'longitude','latitude', 'geometry', 'geo_id', \n",
    "   'STUSPS', 'NAMELSADCO', 'score_cat', 'snpp_avg_dist_km', 'geo_id','geometry'\n",
    "  ]\n",
    "ml_df = model_joined.copy().drop(columns=drop_col)\n",
    "del(model_joined)\n",
    "ml_df._consolidate_inplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b86edc-61bc-4b7b-a7a0-e5604a5ee5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target columns\n",
    "target_col = ml_df['sat_avg']\n",
    "feature_cols = ml_df.drop(columns=['sat_avg'])\n",
    "\n",
    "random_state = 77\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_cols, target_col, test_size=0.2, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886d78d-1b6e-4263-b12a-33f8d046136f",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758a6fc-a2b0-4844-9262-c473c17a48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfr_rand_fp =os.path.join(\"Models\",\"model_pred_best_RFR_randomCV.sav\")\n",
    "\n",
    "# if not os.path.exists(rfr_rand_fp):\n",
    "#     param_dist = {\n",
    "#         \"n_estimators\":    randint(100, 1000),   \n",
    "#         \"max_depth\":       randint(5, 50),       \n",
    "#         \"min_samples_split\": randint(2, 11),    \n",
    "#         \"min_samples_leaf\":  randint(1, 5), \n",
    "#         \"max_features\":    [ \"sqrt\", \"log2\"] \n",
    "#     }\n",
    "    \n",
    "#     rfr = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    "    \n",
    "#     random_srch = RandomizedSearchCV(\n",
    "#         estimator=rfr,\n",
    "#         param_distributions=param_dist,\n",
    "#         n_iter=5,  # start with 20 to get a feel for time\n",
    "#         scoring='neg_mean_squared_error',\n",
    "#         cv=5, \n",
    "#         random_state=random_state,\n",
    "#         n_jobs=-1,\n",
    "#         verbose=5  # 1\n",
    "#     )\n",
    "\n",
    "#     random_srch.fit(X_train, y_train)\n",
    "    \n",
    "#     print('best rfr params:', random_srch.best_params_)\n",
    "#     # print('best score:', -random_srch.best_score_)\n",
    "#     best_rfr = random_srch.best_estimator_\n",
    "    \n",
    "\n",
    "#     pickle.dump(best_rfr, open(rfr_rand_fp, 'wb'))\n",
    "# best_rfr = pickle.load(open(rfr_rand_fp,'rb'))\n",
    "\n",
    "# y_pred = best_rfr.predict(X_train)\n",
    "# print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "\n",
    "# y_pred = best_rfr.predict(X_test)\n",
    "# print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d24f38-5694-4ad1-801e-a5ebd0b50f6d",
   "metadata": {},
   "source": [
    "#### XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e4b0b-e353-47d8-937e-5451a9aa5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_choice = 3\n",
    "n_job_choice = 20\n",
    "\n",
    "xgb_rand_fp =os.path.join(\"Models\",\"model_pred_bbest_XGB_randomCV_{}cv_{}job.sav\".format(cv_choice, n_job_choice))\n",
    "\n",
    "if not os.path.exists(xgb_rand_fp):\n",
    "    \n",
    "    # XGBRegressor\n",
    "    param_dist = {\n",
    "        'n_estimators':randint(100, 1000),\n",
    "        'learning_rate':uniform(0.01, 0.29),\n",
    "        'max_depth':randint(3, 12),\n",
    "        'min_child_weight':randint(1, 10),\n",
    "        'subsample':uniform(0.5, 0.5),\n",
    "        'colsample_bytree':uniform(0.5, 0.5),\n",
    "        'gamma':uniform(0, 0.5),\n",
    "        'reg_alpha': uniform(0, 1),\n",
    "        'reg_lambda':uniform(0, 1),\n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=random_state, n_jobs=-1)\n",
    "    \n",
    "    random_srch = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,  # start with 20 to get a feel for time\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3, #5, \n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbose=5  # 1\n",
    "    )\n",
    "    \n",
    "    random_srch.fit(X_train, y_train)\n",
    "    print('best xgb params:', random_srch.best_params_)\n",
    "    # print('best score:', -random_srch.best_score_)\n",
    "    best_xgb = random_srch.best_estimator_\n",
    "    pickle.dump(best_xgb, open(xgb_rand_fp, 'wb'))\n",
    "    \n",
    "best_xgb = pickle.load(open(xgb_rand_fp,'rb'))\n",
    "y_pred = best_xgb.predict(X_train)\n",
    "print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7198993-04ac-4bfd-b263-7c48227522e6",
   "metadata": {},
   "source": [
    "#### Extract Feature Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02372438-c54c-4e77-8a0e-3cd75cc5de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "booster = best_xgb.get_booster()\n",
    "\n",
    "importance_dict = booster.get_score(importance_type='gain')\n",
    "\n",
    "imp_series = (\n",
    "    pd.Series(importance_dict)\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "imp_arr = pd.Series(best_xgb.feature_importances_, index=X_train.columns)\n",
    "top10 = imp_arr.sort_values(ascending=False).head(10)\n",
    "print(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300ce2a-7c3b-41dd-bd32-a0024b29b4e1",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97417d8-d82a-4d7b-84dd-6d3ea46c2866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
