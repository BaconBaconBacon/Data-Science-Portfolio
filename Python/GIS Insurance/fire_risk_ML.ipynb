{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb6e866-9607-4454-a8a7-8026a7934a6a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "GOAL: To train a model to predict the wild fire risk of properties, using census data as input features and the proximity to fires as a target feature.\n",
    "\n",
    "It would be cool to add in data regarding local climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c435b1fb-b3b7-4d5b-837b-9df9ab020f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdcf5\\Documents\\Data Science Portfolio\\Python\\GIS Insurance\\load_census.py:7: UserWarning: Mapping functions unavailable due to import error: NameError. To use mapping features, ensure all dependencies are properly installed: pip install pytidycensus[map]\n",
      "  import pytidycensus as tc\n"
     ]
    }
   ],
   "source": [
    "import census\n",
    "from census import Census\n",
    "\n",
    "import censusgeocode as cg\n",
    "# import pytidycensus as tc\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import sqlalchemy as sql\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from censusdis.states import ALL_STATES_AND_DC\n",
    "\n",
    "\n",
    "import load_GIS \n",
    "import load_census\n",
    "import load_properties\n",
    "\n",
    "\n",
    "from censusgeocode import CensusGeocode\n",
    "from random_address import real_random_address\n",
    "from sqlalchemy.engine import URL\n",
    "\n",
    "\n",
    "from IPython.display import IFrame\n",
    "# from pygris import tracts\n",
    "from matplotlib.colors import to_hex\n",
    "from scipy.stats import randint, uniform\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "from shapely import distance\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import make_scorer, mean_poisson_deviance, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "CRS = 5070\n",
    "# ALL_STATES_AND_DC\n",
    "\n",
    "\n",
    "engine = sql.create_engine('sqlite:///wildfire_risk_project.sqlite')\n",
    "conn = engine.connect() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da31db-e875-4578-8aa9-eadf78bcf2a2",
   "metadata": {},
   "source": [
    "# Load Training and Target Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5cb6e-f596-446d-90be-3625594ea51d",
   "metadata": {},
   "source": [
    "## Select Properties of Interest\n",
    "\n",
    "Chosing 300000 properties randomly from US addresses. We will join relevant census data to these addresses. This will probably take awhile, so best to run it overnight.\n",
    "\n",
    "We don't care about the address itself. We add a census identifier called the GEOID which based on the coordinate's state, county, and tract number.\n",
    "\n",
    "Using a package that makes use of the [US Census Geocoder API](https://www.census.gov/programs-surveys/geography/technical-documentation/complete-technical-documentation/census-geocoder.html), requests can be in batches of 10,000.\n",
    "\n",
    "https://pypi.org/project/random-address/\n",
    "\n",
    "TODO: This feels like a very naive approach, atm. Takes about 2s per address. Yuck. Speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62bd8da5-5d4b-40cd-8958-a05bd52cf4ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OperationalError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1967\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine._has_events:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:951\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOperationalError\u001b[39m: no such table: properties",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Data Science Portfolio\\Python\\GIS Insurance\\load_properties.py:86\u001b[39m, in \u001b[36mProperties.connect_to_sql\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     geo_df =  \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_postgis\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSELECT * FROM properties\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msql_conn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\geopandas\\io\\sql.py:185\u001b[39m, in \u001b[36m_read_postgis\u001b[39m\u001b[34m(sql, con, geom_col, crs, index_col, coerce_float, parse_dates, params, chunksize)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# read all in one chunk and return a single GeoDataFrame\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _df_to_geodf(df, geom_col=geom_col, crs=crs, con=con)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\pandas\\io\\sql.py:736\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\pandas\\io\\sql.py:1848\u001b[39m, in \u001b[36mSQLDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   1802\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[33;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[32m   1804\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1846\u001b[39m \n\u001b[32m   1847\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1849\u001b[39m columns = result.keys()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\pandas\\io\\sql.py:1671\u001b[39m, in \u001b[36mSQLDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   1670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1671\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_driver_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.con.execute(sql, *args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1779\u001b[39m, in \u001b[36mConnection.exec_driver_sql\u001b[39m\u001b[34m(self, statement, parameters, execution_options)\u001b[39m\n\u001b[32m   1778\u001b[39m dialect = \u001b[38;5;28mself\u001b[39m.dialect\n\u001b[32m-> \u001b[39m\u001b[32m1779\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_init_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1846\u001b[39m, in \u001b[36mConnection._execute_context\u001b[39m\u001b[34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1846\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1986\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1985\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2355\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception\u001b[39m\u001b[34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[39m\n\u001b[32m   2354\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2356\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1967\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine._has_events:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GIS\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:951\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOperationalError\u001b[39m: (sqlite3.OperationalError) no such table: properties\n[SQL: SELECT * FROM properties]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m properties = \u001b[43mload_properties\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProperties\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_engine\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Data Science Portfolio\\Python\\GIS Insurance\\load_properties.py:24\u001b[39m, in \u001b[36mProperties.__init__\u001b[39m\u001b[34m(self, sql_engine, sql_conn)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.sql_engine = sql_engine\n\u001b[32m     23\u001b[39m \u001b[38;5;28mself\u001b[39m.sql_conn = sql_conn\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mself\u001b[39m.properties_gpd = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mself\u001b[39m.num_properties = -\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Data Science Portfolio\\Python\\GIS Insurance\\load_properties.py:87\u001b[39m, in \u001b[36mProperties.connect_to_sql\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     86\u001b[39m     geo_df =  gpd.read_postgis(sql=\u001b[33m'\u001b[39m\u001b[33mSELECT * FROM properties\u001b[39m\u001b[33m'\u001b[39m, con=\u001b[38;5;28mself\u001b[39m.sql_conn)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mOperationalError\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# if not, connect\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m#  'geometry'\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 'geoid'\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# 'county_id'\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# 'state_id'\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'OperationalError' is not defined"
     ]
    }
   ],
   "source": [
    "properties = load_properties.Properties(sql_engine = engine, sql_conn=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518aea75-14ee-42fe-8f34-cc7e12f76fa6",
   "metadata": {},
   "source": [
    "## Load 2023 US Census Data\n",
    "\n",
    "Using an API key, we will use the 'census' Python package to interact with the US Govermnent's census API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10e3293c-3d65-4812-99e4-441955b2e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "census = load_census.CensusData(engine, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939442c0-680d-4b3d-a5bd-f6c54e2100b4",
   "metadata": {},
   "source": [
    "## Load Wildfire GIS Data for 2024\n",
    "\n",
    "We will use point data from the Visible Infrared Imaging Radiometer Suite (VIIRS). A valid alternative is using burn boundary data. There are a few different data sources we could use, but in the interest of (portfolio) simplicity we'll use just the VIIRS.\n",
    "\n",
    "N:B: May be a good chance to practice using AWS DB storage and retrieval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18aaba-9311-40e8-b684-ce626f6f42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_filepaths=[]\n",
    "wf_gis = load_GIS.GISData(engine, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99afe9c-9916-458e-9b66-f4b7f18179c0",
   "metadata": {},
   "source": [
    "## Give Each Property a Wildfire Risk Score\n",
    "\n",
    "Will be based on the proximity to wildfire points, weighted by the number of nearby fires, with a cutoff of 50km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a7a0f-9c28-4216-9fb1-554c57395a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0e0ab-8d9c-4656-88b6-42cf3679b945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fcaae-5c84-468e-8380-f14d68ba4e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc078c43-577f-4069-a203-927b88c373e2",
   "metadata": {},
   "source": [
    "# Machine Learning Considerations\n",
    "#### Scoring Methods\n",
    "\n",
    "For the float risk score, we can use Mean Squared Error (MSE) or Root Mean Squared Error (RMSE). Since it's quadratic in difference between observations and predictions deviations, MSE strongly penalizes large misses, which would be expensive for the insurance company.\n",
    "\n",
    "For the risk category counts, they appear to be Poisson distributed, so a Poisson loss-function is appropriate.\n",
    "\n",
    "For any classification model with the binned risk categories, we want to make large misses costly (i.e. predicting a 1 when the category is a 10), since these would also be very costly to the insurance company. To be honest, MSE will work here as well, since the categories are just "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a12bd-9f3a-45a3-9b13-0287ff19c033",
   "metadata": {},
   "source": [
    "# Model Machine Learning\n",
    "\n",
    "\n",
    "NB: A good chance to make use of AWS compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25654e8f-fc82-48df-b55c-27c50f0ffab6",
   "metadata": {},
   "source": [
    "### Split Data into Features/Targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b244b-0cbd-4df6-bcb1-a262e5cde235",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col =[\n",
    "    'within_MTBS', 'within_MADIS', 'within_WFIGS',\n",
    "   'within_ba', 'within_ba_no_WFIGS', 'noaa20_score',\n",
    "   'noaa20_count_within_radius', 'noaa20_avg_dist_km', \n",
    "   'snpp_score', 'risk_score_prediction', 'snpp_count_within_radius',\n",
    "   'id', 'longitude','latitude', 'geometry', 'geo_id', \n",
    "   'STUSPS', 'NAMELSADCO', 'score_cat', 'snpp_avg_dist_km', 'geo_id','geometry'\n",
    "  ]\n",
    "ml_df = model_joined.copy().drop(columns=drop_col)\n",
    "del(model_joined)\n",
    "ml_df._consolidate_inplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b86edc-61bc-4b7b-a7a0-e5604a5ee5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target columns\n",
    "target_col = ml_df['sat_avg']\n",
    "feature_cols = ml_df.drop(columns=['sat_avg'])\n",
    "\n",
    "random_state = 77\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_cols, target_col, test_size=0.2, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886d78d-1b6e-4263-b12a-33f8d046136f",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758a6fc-a2b0-4844-9262-c473c17a48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfr_rand_fp =os.path.join(\"Models\",\"model_pred_best_RFR_randomCV.sav\")\n",
    "\n",
    "# if not os.path.exists(rfr_rand_fp):\n",
    "#     param_dist = {\n",
    "#         \"n_estimators\":    randint(100, 1000),   \n",
    "#         \"max_depth\":       randint(5, 50),       \n",
    "#         \"min_samples_split\": randint(2, 11),    \n",
    "#         \"min_samples_leaf\":  randint(1, 5), \n",
    "#         \"max_features\":    [ \"sqrt\", \"log2\"] \n",
    "#     }\n",
    "    \n",
    "#     rfr = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    "    \n",
    "#     random_srch = RandomizedSearchCV(\n",
    "#         estimator=rfr,\n",
    "#         param_distributions=param_dist,\n",
    "#         n_iter=5,  # start with 20 to get a feel for time\n",
    "#         scoring='neg_mean_squared_error',\n",
    "#         cv=5, \n",
    "#         random_state=random_state,\n",
    "#         n_jobs=-1,\n",
    "#         verbose=5  # 1\n",
    "#     )\n",
    "\n",
    "#     random_srch.fit(X_train, y_train)\n",
    "    \n",
    "#     print('best rfr params:', random_srch.best_params_)\n",
    "#     # print('best score:', -random_srch.best_score_)\n",
    "#     best_rfr = random_srch.best_estimator_\n",
    "    \n",
    "\n",
    "#     pickle.dump(best_rfr, open(rfr_rand_fp, 'wb'))\n",
    "# best_rfr = pickle.load(open(rfr_rand_fp,'rb'))\n",
    "\n",
    "# y_pred = best_rfr.predict(X_train)\n",
    "# print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "\n",
    "# y_pred = best_rfr.predict(X_test)\n",
    "# print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d24f38-5694-4ad1-801e-a5ebd0b50f6d",
   "metadata": {},
   "source": [
    "#### XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e4b0b-e353-47d8-937e-5451a9aa5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_choice = 3\n",
    "n_job_choice = 20\n",
    "\n",
    "xgb_rand_fp =os.path.join(\"Models\",\"model_pred_bbest_XGB_randomCV_{}cv_{}job.sav\".format(cv_choice, n_job_choice))\n",
    "\n",
    "if not os.path.exists(xgb_rand_fp):\n",
    "    \n",
    "    # XGBRegressor\n",
    "    param_dist = {\n",
    "        'n_estimators':randint(100, 1000),\n",
    "        'learning_rate':uniform(0.01, 0.29),\n",
    "        'max_depth':randint(3, 12),\n",
    "        'min_child_weight':randint(1, 10),\n",
    "        'subsample':uniform(0.5, 0.5),\n",
    "        'colsample_bytree':uniform(0.5, 0.5),\n",
    "        'gamma':uniform(0, 0.5),\n",
    "        'reg_alpha': uniform(0, 1),\n",
    "        'reg_lambda':uniform(0, 1),\n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=random_state, n_jobs=-1)\n",
    "    \n",
    "    random_srch = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,  # start with 20 to get a feel for time\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3, #5, \n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbose=5  # 1\n",
    "    )\n",
    "    \n",
    "    random_srch.fit(X_train, y_train)\n",
    "    print('best xgb params:', random_srch.best_params_)\n",
    "    # print('best score:', -random_srch.best_score_)\n",
    "    best_xgb = random_srch.best_estimator_\n",
    "    pickle.dump(best_xgb, open(xgb_rand_fp, 'wb'))\n",
    "    \n",
    "best_xgb = pickle.load(open(xgb_rand_fp,'rb'))\n",
    "y_pred = best_xgb.predict(X_train)\n",
    "print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7198993-04ac-4bfd-b263-7c48227522e6",
   "metadata": {},
   "source": [
    "#### Extract Feature Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02372438-c54c-4e77-8a0e-3cd75cc5de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "booster = best_xgb.get_booster()\n",
    "\n",
    "importance_dict = booster.get_score(importance_type='gain')\n",
    "\n",
    "imp_series = (\n",
    "    pd.Series(importance_dict)\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "imp_arr = pd.Series(best_xgb.feature_importances_, index=X_train.columns)\n",
    "top10 = imp_arr.sort_values(ascending=False).head(10)\n",
    "print(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300ce2a-7c3b-41dd-bd32-a0024b29b4e1",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97417d8-d82a-4d7b-84dd-6d3ea46c2866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
